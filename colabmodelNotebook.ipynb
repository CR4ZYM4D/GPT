{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CR4ZYM4D/GPT/blob/master/colabmodelNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c565eb18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c565eb18",
        "outputId": "65c8fad1-8ab6-4833-82ac-1d4bb97e1822"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import mmap\n",
        "import random\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "#using GPU if available\n",
        "drive.mount('/content/drive')\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f7d51dd7",
      "metadata": {
        "id": "f7d51dd7"
      },
      "outputs": [],
      "source": [
        "# important constants to be used in the model\n",
        "\n",
        "block_size = 1026 # size of a single word or a combination of words (we will refer to this a s a block)\n",
        "\n",
        "batch_size = 12 # no. of said blocks or words that we will handle at once\n",
        "\n",
        "vector_dimension = 512 # dimensions of each of the alphabet or token vector\n",
        "\n",
        "dropout = 0.5\n",
        "\n",
        "n_heads = 12 # no of attention heads\n",
        "\n",
        "n_layers = 6 # no of block layers used\n",
        "\n",
        "max_sequence_length = 1026 # max no of tokens that will be generated\n",
        "\n",
        "learning_rate = 3e-4\n",
        "\n",
        "max_iterations = 2000\n",
        "\n",
        "train_step_iteration = max_iterations/10\n",
        "\n",
        "max_test_iterations = 500\n",
        "\n",
        "test_iterations = 10\n",
        "\n",
        "test_step_iterations = 50\n",
        "\n",
        "model_path = './content/models/nb/colabmodel.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "08394d7b",
      "metadata": {
        "id": "08394d7b"
      },
      "outputs": [],
      "source": [
        "# function to read a text file and return all characters present in it\n",
        "\n",
        "def readTextFile(path):\n",
        "\n",
        "    with open(path, 'r', encoding = 'UTF-8') as f:\n",
        "\n",
        "        text = f.read()\n",
        "\n",
        "    characters = sorted(set(text))\n",
        "\n",
        "    return characters\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    !unzip \"/content/drive/My Drive/training data.zip\" -d \"/content/training_data\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPEwvkqZyjvi",
        "outputId": "4ddfd8f7-453e-4b8b-bb25-17a82c55b398"
      },
      "id": "yPEwvkqZyjvi",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/My Drive/training data.zip\n",
            "replace /content/training_data/training data.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "f308aada",
      "metadata": {
        "id": "f308aada"
      },
      "outputs": [],
      "source": [
        "# function to get a chunk from memory of training/testing data\n",
        "\n",
        "def getChunk(split: str, i):\n",
        "\n",
        "    file_path = '/content/training_data/training data.txt' if split == 'train' else './dataset/testing data.txt'\n",
        "\n",
        "    with open(file_path, \"rb\") as f:\n",
        "\n",
        "        with mmap.mmap(f.fileno(), 0, access = mmap.ACCESS_READ) as mm:\n",
        "\n",
        "            file_size = len(mm)\n",
        "\n",
        "            start_pos = i\n",
        "\n",
        "            mm.seek(start_pos)\n",
        "\n",
        "            block = mm.read(block_size * batch_size)\n",
        "\n",
        "            decoded_block = block.decode(encoding = \"utf-8\", errors = \"ignore\").replace(\"\\r\", \"\")\n",
        "\n",
        "            data = tokenizer.encode(text = decoded_block, add_special_tokens= True, return_tensors = 'pt').aslist()\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "aefc0f1e",
      "metadata": {
        "id": "aefc0f1e"
      },
      "outputs": [],
      "source": [
        "# getting the batch from either of the splits. Training split by default\n",
        "\n",
        "def getBatch(split = \"train\", idx = 0):\n",
        "\n",
        "    data = getChunk(split, idx)\n",
        "\n",
        "    index = torch.randint(0, block_size -1, size = (batch_size,))\n",
        "\n",
        "    x = torch.stack([data[i : i + block_size] for i in index])\n",
        "    y = torch.stack([data[i+1: i+block_size+1] for i in index])\n",
        "\n",
        "    # Add assertions to check the range of target indices\n",
        "    assert torch.all(y >= 0), \"Target indices contain negative values.\"\n",
        "    assert torch.all(y < tokenizer.vocab_size), f\"Target indices are out of vocabulary range. Max target index: {torch.max(y)}, Vocab size: {tokenizer.vocab_size}\"\n",
        "\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "57237709",
      "metadata": {
        "id": "57237709"
      },
      "outputs": [],
      "source": [
        "\n",
        "# class that performs the feed forward mechanism of the decoder block (class that normalizes the vectors, aplies Relu and again normalizes them)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, vector_dimension):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(vector_dimension, vector_dimension * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(vector_dimension * 4, vector_dimension)\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.dropout(self.layer(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9a2a77db",
      "metadata": {
        "id": "9a2a77db"
      },
      "outputs": [],
      "source": [
        "# class for the single head self attention\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "\n",
        "    def __init__(self, dimension_head):\n",
        "        super().__init__()\n",
        "\n",
        "        self.key = nn.Linear(vector_dimension, dimension_head, bias = False)\n",
        "        self.value = nn.Linear(vector_dimension, dimension_head, bias = False)\n",
        "        self.query = nn.Linear(vector_dimension, dimension_head, bias = False)\n",
        "\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch, time, channels = x.shape\n",
        "\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) / (k.shape[-1] ** 0.5)\n",
        "\n",
        "        att = att.masked_fill(self.tril[:time, :time] == 0, float('-inf'))\n",
        "\n",
        "        att = F.softmax(att, dim = -1)\n",
        "        att = self.dropout(att)\n",
        "        out = att @ v\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "16dc58a1",
      "metadata": {
        "id": "16dc58a1"
      },
      "outputs": [],
      "source": [
        "# class for the multi head attention mechanism\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, n_heads, dimension_head):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention_heads = nn.ModuleList([AttentionHead(dimension_head) for _ in range (n_heads)])\n",
        "\n",
        "        self.projection_layer = nn.Linear(n_heads * dimension_head, vector_dimension)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = torch.cat([h(x) for h in self.attention_heads], dim =-1)\n",
        "\n",
        "        out = self.dropout(self.projection_layer(out))\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "417f25a5",
      "metadata": {
        "id": "417f25a5"
      },
      "outputs": [],
      "source": [
        "# class of a single decoder block\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_heads, vector_dimension):\n",
        "        super().__init__()\n",
        "\n",
        "        dimension_head = vector_dimension // n_heads\n",
        "\n",
        "        self.attention = MultiHeadAttention(n_heads, dimension_head)\n",
        "\n",
        "        self.feed_forward = FeedForward(vector_dimension)\n",
        "\n",
        "        self.layer_norm_1 = nn.LayerNorm(vector_dimension)\n",
        "\n",
        "        self.layer_norm_2 = nn.LayerNorm(vector_dimension)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        y = self.attention(x)\n",
        "\n",
        "        x = self.layer_norm_1(x + y)\n",
        "\n",
        "        y = self.feed_forward(x)\n",
        "\n",
        "        x = self.layer_norm_2(x + y)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "88c2901f",
      "metadata": {
        "id": "88c2901f"
      },
      "outputs": [],
      "source": [
        "# Building the model class\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "\n",
        "    # constructor\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.token_embeddings = nn.Embedding(vocab_size, vector_dimension)\n",
        "\n",
        "        self.positional_encodings = nn.Embedding(block_size, vector_dimension)\n",
        "\n",
        "        self.layers = nn.Sequential(*[Block(n_heads, vector_dimension) for _ in range (n_layers)])\n",
        "\n",
        "        self.final_layer_norm = nn.LayerNorm(vector_dimension)\n",
        "\n",
        "        self.linear = nn.Linear(vector_dimension, vocab_size)\n",
        "\n",
        "        self.apply(self.initWeights)\n",
        "\n",
        "    # method to initialize the weights\n",
        "\n",
        "    def initWeights(self, module):\n",
        "\n",
        "        if isinstance(module, nn.Linear):\n",
        "\n",
        "            torch.nn.init.normal_(module.weight, std = 0.02)\n",
        "\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "\n",
        "            torch.nn.init.normal_(module.weight, std = 0.02)\n",
        "\n",
        "    # method to forward the next token\n",
        "\n",
        "    def forward(self, index, targets = None):\n",
        "\n",
        "        batch, time = index.shape\n",
        "\n",
        "        token_embedding = self.token_embeddings(index)\n",
        "\n",
        "        positional_encoding = self.positional_encodings(torch.arange(time, device = device))\n",
        "\n",
        "        x = token_embedding + positional_encoding\n",
        "\n",
        "        x = self.layers(x)\n",
        "\n",
        "        x = self.final_layer_norm(x)\n",
        "\n",
        "        logits = self.linear(x)\n",
        "\n",
        "        if targets == None:\n",
        "\n",
        "            loss = None\n",
        "\n",
        "        else:\n",
        "\n",
        "            batch, time, channels = logits.shape\n",
        "\n",
        "            logits = logits.view(batch * time, channels)\n",
        "\n",
        "            targets = targets.view(batch * time)\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    # method to generate the tokens\n",
        "\n",
        "    def generate(self, index, max_sequence_length):\n",
        "\n",
        "        result = torch.clone(index)\n",
        "\n",
        "        for _ in range(max_sequence_length):\n",
        "\n",
        "            logits, loss = self.forward(index)\n",
        "\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            probabilities = F.softmax(logits, dim = -1)\n",
        "\n",
        "            next_index = torch.multinomial(probabilities, num_samples = 1)\n",
        "\n",
        "            index = torch.cat((index, next_index), dim = 1)\n",
        "\n",
        "            result = torch.cat((index, next_index), dim = 1)\n",
        "\n",
        "            a, b = index.shape\n",
        "\n",
        "            if b >= block_size:\n",
        "                index = index[:, 1: ]\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b50357fa",
      "metadata": {
        "id": "b50357fa"
      },
      "outputs": [],
      "source": [
        "# method to optimize and train the model\n",
        "\n",
        "def train(model: GPTModel, index):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "\n",
        "    avg_training_losses = []\n",
        "\n",
        "    training_losses = []\n",
        "\n",
        "    sum = 0\n",
        "\n",
        "    for i in range (max_iterations):\n",
        "\n",
        "        x, y = getBatch(\"train\", index)\n",
        "\n",
        "        logits, loss = model.forward(x, y)\n",
        "\n",
        "        sum += loss.item()\n",
        "\n",
        "        training_losses.append(loss.item())\n",
        "\n",
        "        avg_training_losses.append(sum/(i+1))\n",
        "\n",
        "        optimizer.zero_grad(set_to_none = True)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % train_step_iteration == 0 :\n",
        "\n",
        "            print(f\"training loss at step {i+1} is: {loss.item(): .5f}\")\n",
        "            print(f\"average training loss at step {i+1} is: {avg_training_losses[-1]: .5f}\")\n",
        "\n",
        "    # plt.scatter(np.arange(0, max_iterations), avg_training_losses)\n",
        "    # plt.title(\" average training data loss in n loops v/s num loops\")\n",
        "    # plt.xticks(np.arange(0, max_iterations+1, 1000))\n",
        "    # plt.yticks(np.arange(0, 2.1, 0.1))\n",
        "    # plt.grid(True)\n",
        "    # plt.savefig(f\"../graphs/nb/avg_training loss{learning_rate}.jpeg\")\n",
        "    # plt.show()\n",
        "\n",
        "    # plt.scatter(np.arange(0, max_iterations), training_losses)\n",
        "    # plt.title(\"training data loss in n loops v/s num loops\")\n",
        "    # plt.xticks(np.arange(0, max_iterations+1, 1000))\n",
        "    # plt.yticks(np.arange(0, 10.1, 0.5))\n",
        "    # plt.grid(True)\n",
        "    # plt.savefig(f\"../graphs/nb/training loss{learning_rate}.jpeg\")\n",
        "    # plt.show()\n",
        "\n",
        "    torch.save(model, model_path)\n",
        "    print(\"saved model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "697f432d",
      "metadata": {
        "id": "697f432d"
      },
      "outputs": [],
      "source": [
        "# method to calculate the loss\n",
        "\n",
        "@torch.no_grad()\n",
        "\n",
        "def calculateLoss(model):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    out = {}\n",
        "\n",
        "    splits = [\"train\", \"test\"]\n",
        "\n",
        "    for split in splits:\n",
        "\n",
        "        losses = torch.zeros(test_iterations)\n",
        "\n",
        "        for iteration in range (test_iterations):\n",
        "\n",
        "            x, y = getBatch(split)\n",
        "\n",
        "            logits, loss = model.forward(x, y)\n",
        "\n",
        "            losses[iteration] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean()\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "dcc625c5",
      "metadata": {
        "id": "dcc625c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "8d50383b-fd40-40e1-fc3b-7e78bb53988f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-55-135214175.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mGPTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "model = torch.load(model_path, weights_only= False) if os.path.exists(model_path) else GPTModel(vocab_size)\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "with open ('/content/training_data/training data.txt', 'r', encoding = 'utf-8') as f:\n",
        "  i =0\n",
        "  while f.readline() :\n",
        "\n",
        "    train(model, i)\n",
        "\n",
        "    i = i + block_size * batch_size\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for i in range(max_test_iterations):\n",
        "\n",
        "    loss = calculateLoss(model)\n",
        "\n",
        "    train_losses.append(loss['train'])\n",
        "    test_losses.append(loss['test'])\n",
        "\n",
        "    if (i+1) % test_step_iterations == 0:\n",
        "\n",
        "        print( f\"at step {i+1} training loss is: {loss['train']: .5f} and testing loss is: {loss['test']: .5f}\")\n",
        "\n",
        "# plt.scatter(np.arange(1, max_test_iterations+1), train_losses, color = \"r\", label = \"training set\")\n",
        "# plt.scatter(np.arange(1, max_test_iterations+1), test_losses, color = \"g\", label = \"testing set\")\n",
        "\n",
        "# plt.xticks(np.arange(0, max_test_iterations+1, 2 * test_iterations))\n",
        "# plt.yticks(np.arange(0, 2, 0.05))\n",
        "\n",
        "# plt.xlabel(\"loop num.\")\n",
        "# plt.ylabel(\"data loss\")\n",
        "\n",
        "# plt.title(\"loss when testing\")\n",
        "# plt.grid(True)\n",
        "# plt.legend()\n",
        "# plt.savefig(f\"../graphs/nb/testing loss{learning_rate}.jpeg\")\n",
        "\n",
        "while (True):\n",
        "\n",
        "    prompt = input(\"Enter a prompt\")\n",
        "\n",
        "    context = tokenizer.encode(text = prompt,add_special_tokens= True, return_tensors = 'pt', device= device )\n",
        "\n",
        "    context = context.unsqueeze(0)\n",
        "\n",
        "    generated_chars = tokenizer.decode(model.generate(context, max_sequence_length))[0].tolist()\n",
        "\n",
        "    print(generated_chars)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}