{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c565eb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import mmap\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#using GPU if available\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d51dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important constants to be used in the model\n",
    "\n",
    "block_size = 128 # size of a single word or a combination of words (we will refer to this a s a block)\n",
    "\n",
    "batch_size = 48 # no. of said blocks or words that we will handle at once\n",
    "\n",
    "vector_dimension = 512 # dimensions of each of the alphabet or token vector\n",
    "\n",
    "dropout = 0.5\n",
    "\n",
    "n_heads = 12 # no of attention heads\n",
    "\n",
    "n_layers = 6 # no of block layers used \n",
    "\n",
    "max_sequence_length = 400 # max no of tokens that will be generated\n",
    "\n",
    "learning_rate = 3e-4\n",
    "\n",
    "max_iterations = 10000\n",
    "\n",
    "train_step_iteration = max_iterations/10\n",
    "\n",
    "max_test_iterations = 200\n",
    "\n",
    "test_iterations = 10\n",
    "\n",
    "test_step_iterations = 5\n",
    "\n",
    "model_path = './models/model2.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08394d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read a text file and return all characters present in it\n",
    "\n",
    "def readTextFile(path):\n",
    "\n",
    "    with open(path, 'r', encoding = 'UTF-8') as f:\n",
    "\n",
    "        text = f.read()\n",
    "\n",
    "    characters = sorted(set(text))\n",
    "\n",
    "    return characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = readTextFile(\"./dataset/vocab.txt\")\n",
    "\n",
    "# getting vocabulary size\n",
    "\n",
    "vocab_size = len(characters)\n",
    "\n",
    "# lambda functions to encode and decode the characters of the text\n",
    "\n",
    "stringToNum = {ch: i for i, ch  in enumerate(characters)}\n",
    "\n",
    "numToString = {i: ch for i,ch in enumerate(characters)}\n",
    "\n",
    "encode = lambda s: [stringToNum[c] for c in s]\n",
    "\n",
    "decode = lambda s: ''.join(numToString[n] for n in s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get a chunk from memory of training/testing data\n",
    "\n",
    "def getChunk(split):\n",
    "\n",
    "    file_path = './dataset/training data.txt' if split == 'train' else './dataset/testing data.txt' \n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "\n",
    "        with mmap.mmap(f.fileno(), 0, access = mmap.ACCESS_READ) as mm:\n",
    "\n",
    "            file_size = len(mm)\n",
    "\n",
    "            start_pos = random.randint(0, file_size - block_size * batch_size -1)\n",
    "\n",
    "            mm.seek(start_pos)\n",
    "\n",
    "            block = mm.read(block_size * batch_size)\n",
    "\n",
    "            decoded_block = block.decode(encoding = \"utf-8\", errors = \"ignore\").replace(\"\\r\", \"\")\n",
    "\n",
    "            data = torch.tensor(encode(decoded_block), dtype = torch.long)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefc0f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the batch from either of the splits. Training split by default\n",
    "\n",
    "def getBatch(split = \"train\"):\n",
    "\n",
    "    data = getChunk(split)\n",
    "    \n",
    "    index = torch.randint(0, len(data) - block_size -1, size = (batch_size,))\n",
    "\n",
    "    x = torch.stack([data[i : i + block_size] for i in index])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in index])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57237709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class that performs the feed forward mechanism of the decoder block (class that normalizes the vectors, aplies Relu and again normalizes them)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, vector_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(vector_dimension, vector_dimension * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(vector_dimension * 4, vector_dimension)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.dropout(self.layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the single head self attention\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, dimension_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = nn.Linear(vector_dimension, dimension_head, bias = False)\n",
    "        self.value = nn.Linear(vector_dimension, dimension_head, bias = False) \n",
    "        self.query = nn.Linear(vector_dimension, dimension_head, bias = False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch, time, channels = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / (k.shape[-1] ** 0.5)\n",
    "\n",
    "        att = att.masked_fill(self.tril[:time, :time] == 0, float('-inf'))\n",
    "\n",
    "        att = F.softmax(att, dim = -1)\n",
    "        att = self.dropout(att)\n",
    "        out = att @ v\n",
    "\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16dc58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the multi head attention mechanism\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, dimension_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention_heads = nn.ModuleList([AttentionHead(dimension_head) for _ in range (n_heads)])\n",
    "\n",
    "        self.projection_layer = nn.Linear(n_heads * dimension_head, vector_dimension)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = torch.cat([h(x) for h in self.attention_heads])\n",
    "\n",
    "        out = self.dropout(self.projection_layer(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "417f25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class of a single decoder block\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, vector_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        dimension_head = vector_dimension // n_heads\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_heads, dimension_head)\n",
    "\n",
    "        self.feed_forward = FeedForward(vector_dimension)\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(vector_dimension)\n",
    "\n",
    "        self.layer_norm_2 = nn.LayerNorm(vector_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        y = self.attention(x)\n",
    "\n",
    "        x = self.layer_norm_1(x + y)\n",
    "\n",
    "        y = self.feed_forward(x)\n",
    "\n",
    "        x = self.layer_norm_2(x + y)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88c2901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model class\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "\n",
    "    # constructor\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, vector_dimension)\n",
    "\n",
    "        self.positional_encodings = nn.Embedding(block_size, vector_dimension)\n",
    "\n",
    "        self.layers = nn.Sequential(*[Block(n_heads, vector_dimension) for _ in range (n_layers)])\n",
    "\n",
    "        self.final_layer_norm = nn.LayerNorm(vector_dimension)\n",
    "\n",
    "        self.linear = nn.Linear(vector_dimension, vocab_size)\n",
    "\n",
    "        self.apply(self.initWeights)\n",
    "\n",
    "    # method to initialize the weights\n",
    "\n",
    "    def initWeights(self, module):\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "\n",
    "            torch.nn.init.normal_(module.weight, std = 0.02)\n",
    "\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "\n",
    "            torch.nn.init.normal_(module.weight, std = 0.02)\n",
    "\n",
    "    # method to forward the next token \n",
    "\n",
    "    def forward(self, index, targets = None):\n",
    "        \n",
    "        batch, time = index.shape\n",
    "\n",
    "        token_embedding = self.token_embeddings(index)\n",
    "\n",
    "        positional_encoding = self.positional_encodings(torch.arange(time, device = device))\n",
    "\n",
    "        x = token_embedding + positional_encoding\n",
    "\n",
    "        x = self.layers(x)\n",
    "\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        if targets == None:\n",
    "\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "\n",
    "            batch, time, channels = logits.shape\n",
    "\n",
    "            logits = logits.view(batch * time, channels)\n",
    "\n",
    "            targets = targets.view(batch * time)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    # method to generate the tokens \n",
    "    \n",
    "    def generate(self, index, max_sequence_length):\n",
    "\n",
    "        for _ in range(max_sequence_length):\n",
    "\n",
    "            logits, loss = self.forward(index)\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probabilities = F.softmax(logits, dim = -1)\n",
    "\n",
    "            next_index = torch.multinomial(probabilities, num_samples = 1)\n",
    "\n",
    "            index = torch.cat((index, next_index), dim = 1)\n",
    "\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50357fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to optimize and train the model\n",
    "\n",
    "def train(model: GPTModel):\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    for i in range (max_iterations):\n",
    "\n",
    "        x, y = getBatch(\"train\")\n",
    "\n",
    "        logits, loss = model.forward(x, y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % train_step_iteration == 0 :\n",
    "\n",
    "            print(loss.item())\n",
    "\n",
    "    torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "697f432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate the loss\n",
    "\n",
    "@torch.no_grad()\n",
    "\n",
    "def calculateLoss(model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    splits = [\"train\", \"test\"]\n",
    "\n",
    "    for split in splits: \n",
    "\n",
    "        losses = torch.zeros(test_iterations)\n",
    "\n",
    "        for iteration in range (test_iterations):\n",
    "\n",
    "            x, y = getBatch(split)\n",
    "\n",
    "            logits, loss = model.forward(x, y)\n",
    "\n",
    "            losses[iteration] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "\n",
    "        model.train()\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc625c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(model_path, weights_only= False) if os.path.exists(model_path) else GPTModel(vocab_size)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "train(model)\n",
    "\n",
    "for i in range(max_test_iterations): \n",
    "\n",
    "    loss = calculateLoss(model)\n",
    "\n",
    "    if i % test_step_iterations == 9:\n",
    "\n",
    "        print( f\"at step {i+1} training loss is: {loss['train']} and testing loss is: {loss['test']}\")\n",
    "\n",
    "# context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "\n",
    "# generated_chars = decode(model.generate(context, max_sequence_length)[0].tolist())\n",
    "\n",
    "# print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
