{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c565eb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#using GPU if available\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d51dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important constants to be used in the model\n",
    "\n",
    "block_size = 9 # size of a single word or a combination of words (we will refer to this a s a block)\n",
    "\n",
    "batch_size = 12 # no. of said blocks or words that we will handle at once\n",
    "\n",
    "vector_dimension = 512 # dimensions of each of the alphabet or token vector\n",
    "\n",
    "dropout = 0.5\n",
    "\n",
    "n_heads = 8 # no of attention heads\n",
    "\n",
    "n_layers = 4 # no of block layers used \n",
    "\n",
    "max_sequence_length = 400 # max no of tokens that will be generated\n",
    "\n",
    "learning_rate = 3e-2\n",
    "\n",
    "max_iterations = 20000\n",
    "\n",
    "train_step_iteration = 10\n",
    "\n",
    "max_test_iterations = 200\n",
    "\n",
    "test_iterations = 20\n",
    "\n",
    "test_step_iterations = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08394d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read a text file and return all characters present in it\n",
    "\n",
    "def readTextFile(path):\n",
    "\n",
    "    with open(path, 'r', encoding = 'UTF-8') as f:\n",
    "\n",
    "        text = f.read()\n",
    "\n",
    "    characters = sorted(set(text))\n",
    "\n",
    "    return text, characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "510f0fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, characters = readTextFile('./src/wizardOfOz.txt')\n",
    "\n",
    "# getting vocabulary size\n",
    "\n",
    "vocab_size = len(characters)\n",
    "\n",
    "# lambda functions to encode and decode the characters of the text\n",
    "\n",
    "stringToNum = {ch: i for i, ch  in enumerate(characters)}\n",
    "\n",
    "numToString = {i: ch for i,ch in enumerate(characters)}\n",
    "\n",
    "encode = lambda s: [stringToNum[c] for c in s]\n",
    "\n",
    "decode = lambda s: ''.join(numToString[n] for n in s)\n",
    "\n",
    "# converting the text into a tensor\n",
    "\n",
    "data = torch.tensor(encode(text), dtype = torch.long)\n",
    "\n",
    "# splitting the data into training and testing sets\n",
    "\n",
    "training_set_size = int(0.8 * len(data))\n",
    "\n",
    "testing_set_size = len(data) - training_set_size\n",
    "\n",
    "training_set = data[:training_set_size]\n",
    "\n",
    "testing_set = data[training_set_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aefc0f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the batch from either of the splits. Training split by default\n",
    "\n",
    "def getBatch(split = \"train\"):\n",
    "\n",
    "    index = torch.randint(high = training_set_size - block_size -1 if split == \"train\" else testing_set_size - block_size - 1, size = (batch_size,))\n",
    "\n",
    "    if split == \"train\":\n",
    "\n",
    "        x = torch.stack([training_set[ix: ix + block_size] for ix in index])\n",
    "\n",
    "        y = torch.stack([training_set[ix + 1: ix + block_size + 1] for ix in index])\n",
    "\n",
    "    else:\n",
    "\n",
    "        x = torch.stack([testing_set[ix: ix + block_size] for ix in index])\n",
    "\n",
    "        y = torch.stack([testing_set[ix + 1: ix + block_size + 1] for ix in index])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57237709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class that performs the feed forward mechanism of the decoder block (class that normalizes the vectors, aplies Relu and again normalizes them)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, vector_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(vector_dimension, vector_dimension * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(vector_dimension * 4, vector_dimension)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.dropout(self.layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class of a single decoder block\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, vector_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        dimension_head = vector_dimension // n_heads\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_heads, dimension_head)\n",
    "\n",
    "        self.feed_forward = FeedForward(vector_dimension)\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(vector_dimension)\n",
    "\n",
    "        self.layer_norm_2 = nn.LayerNorm(vector_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        y = self.attention(x)\n",
    "\n",
    "        x = self.layer_norm_1(x + y)\n",
    "\n",
    "        y = self.feed_forward(x)\n",
    "\n",
    "        x = self.layer_norm_2(x + y)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model class\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "\n",
    "    # constructor\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, vector_dimension)\n",
    "\n",
    "        self.positional_encodings = nn.Embedding(block_size, vector_dimension)\n",
    "\n",
    "        self.layers = nn.Sequential(*[Block(n_heads, vector_dimension) for _ in range (n_layers)])\n",
    "\n",
    "        self.final_layer_norm = nn.LayerNorm(vector_dimension)\n",
    "\n",
    "        self.linear = nn.Linear(vector_dimension, vocab_size)\n",
    "\n",
    "        self.apply(self.initWeights)\n",
    "\n",
    "    # method to initialize the weights\n",
    "\n",
    "    def initWeights(self, module):\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "\n",
    "            torch.nn.init.normal_(module.weight, std = 0.02)\n",
    "\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "\n",
    "            torch.nn.init.normal_(module.weight, std = 0.02)\n",
    "\n",
    "    # method to forward the next token \n",
    "\n",
    "    def forward(self, index, targets = None):\n",
    "        \n",
    "        logits = self.token_embeddings(index)\n",
    "\n",
    "        token_embedding = self.token_embeddings(index)\n",
    "\n",
    "        positional_encoding = self.positional_encodings(index)\n",
    "\n",
    "        x = token_embedding + positional_encoding\n",
    "\n",
    "        x = self.layers(x)\n",
    "\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        if targets == None:\n",
    "\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "\n",
    "            batch, time, channels = logits.shape\n",
    "\n",
    "            logits = logits.view(batch * time, channels)\n",
    "\n",
    "            targets = targets.view(batch * time)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    # method to generate the tokens \n",
    "    \n",
    "    def generate(self, index, max_sequence_length):\n",
    "\n",
    "        for _ in range(max_sequence_length):\n",
    "\n",
    "            logits, loss = self.forward(index)\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probabilities = F.softmax(logits, dim = -1)\n",
    "\n",
    "            next_index = torch.multinomial(probabilities, num_samples = 1)\n",
    "\n",
    "            index = torch.cat((index, next_index), dim = 1)\n",
    "\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b50357fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to optimize and train the model\n",
    "\n",
    "def train(model: GPTModel):\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    for i in range (max_iterations):\n",
    "\n",
    "        x, y = getBatch(\"train\")\n",
    "\n",
    "        logits, loss = model.forward(x, y)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % train_step_iteration == 0 :\n",
    "\n",
    "            print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "697f432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate the loss\n",
    "\n",
    "@torch.no_grad()\n",
    "\n",
    "def calculateLoss(model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    splits = [\"train\", \"test\"]\n",
    "\n",
    "    for split in splits: \n",
    "\n",
    "        losses = torch.zeros(test_iterations)\n",
    "\n",
    "        for iteration in range (test_iterations):\n",
    "\n",
    "            x, y = getBatch(split)\n",
    "\n",
    "            logits, loss = model.forward(x, y)\n",
    "\n",
    "            losses[iteration] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "\n",
    "        model.train()\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc625c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTModel(vocab_size)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "train(model)\n",
    "\n",
    "for i in range(max_test_iterations): \n",
    "\n",
    "    loss = calculateLoss(model)\n",
    "\n",
    "    if i % test_step_iterations == 9:\n",
    "\n",
    "        print( f\"at step {i+1} training loss is: {loss['train']} and testing loss is: {loss['test']}\")\n",
    "\n",
    "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "\n",
    "generated_chars = decode(model.generate(context, max_sequence_length)[0].tolist())\n",
    "\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
