{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c565eb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import mmap\n",
    "import random\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#using GPU if available\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d51dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# important constants to be used in the model\n",
    "\n",
    "block_size = 128 # size of a single word or a combination of words (we will refer to this a s a block)\n",
    "\n",
    "batch_size = 48 # no. of said blocks or words that we will handle at once\n",
    "\n",
    "vector_dimension = 512 # dimensions of each of the alphabet or token vector\n",
    "\n",
    "dropout = 0.5\n",
    "\n",
    "n_heads = 16 # no of attention heads\n",
    "\n",
    "n_layers = 6 # no of block layers used \n",
    "\n",
    "max_sequence_length = 400 # max no of tokens that will be generated\n",
    "\n",
    "learning_rate = 3e-4\n",
    "\n",
    "max_iterations = 8000\n",
    "\n",
    "train_step_iteration = max_iterations/10\n",
    "\n",
    "max_test_iterations = 200\n",
    "\n",
    "test_iterations = 10\n",
    "\n",
    "test_step_iterations = 5\n",
    "\n",
    "model_path = '../models/nb/model.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08394d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read a text file and return all characters present in it\n",
    "\n",
    "def readTextFile(path):\n",
    "\n",
    "    with open(path, 'r', encoding = 'UTF-8') as f:\n",
    "\n",
    "        text = f.read()\n",
    "\n",
    "    characters = sorted(set(text))\n",
    "\n",
    "    return characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "510f0fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32172\n"
     ]
    }
   ],
   "source": [
    "characters = readTextFile(\"../dataset/vocab.txt\")\n",
    "\n",
    "# getting vocabulary size\n",
    "\n",
    "vocab_size = len(characters)\n",
    "\n",
    "print(vocab_size)\n",
    "\n",
    "# lambda functions to encode and decode the characters of the text\n",
    "\n",
    "stringToNum = {ch: i for i, ch  in enumerate(characters)}\n",
    "\n",
    "numToString = {i: ch for i,ch in enumerate(characters)}\n",
    "\n",
    "encode = lambda s: [stringToNum[c] for c in s]\n",
    "\n",
    "decode = lambda s: ''.join(numToString[n] for n in s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f308aada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get a chunk from memory of training/testing data\n",
    "\n",
    "def getChunk(split):\n",
    "\n",
    "    file_path = '../dataset/training data.txt' if split == 'train' else '../dataset/testing data.txt' \n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "\n",
    "        with mmap.mmap(f.fileno(), 0, access = mmap.ACCESS_READ) as mm:\n",
    "\n",
    "            file_size = len(mm)\n",
    "\n",
    "            start_pos = random.randint(0, file_size - block_size * batch_size -1)\n",
    "\n",
    "            mm.seek(start_pos)\n",
    "\n",
    "            block = mm.read(block_size * batch_size)\n",
    "\n",
    "            decoded_block = block.decode(encoding = \"utf-8\", errors = \"ignore\").replace(\"\\r\", \"\")\n",
    "\n",
    "            data = torch.tensor(encode(decoded_block), dtype = torch.long)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aefc0f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the batch from either of the splits. Training split by default\n",
    "\n",
    "def getBatch(split = \"train\"):\n",
    "\n",
    "    data = getChunk(split)\n",
    "    \n",
    "    index = torch.randint(0, len(data) - block_size -1, size = (batch_size,))\n",
    "\n",
    "    x = torch.stack([data[i : i + block_size] for i in index])\n",
    "    y = torch.stack([data[i+1: i+block_size+1] for i in index])\n",
    "\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57237709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class that performs the feed forward mechanism of the decoder block (class that normalizes the vectors, aplies Relu and again normalizes them)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, vector_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(vector_dimension, vector_dimension * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(vector_dimension * 4, vector_dimension)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.dropout(self.layer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a2a77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the single head self attention\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "    def __init__(self, dimension_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = nn.Linear(vector_dimension, dimension_head, bias = False)\n",
    "        self.value = nn.Linear(vector_dimension, dimension_head, bias = False) \n",
    "        self.query = nn.Linear(vector_dimension, dimension_head, bias = False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch, time, channels = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) / (k.shape[-1] ** 0.5)\n",
    "\n",
    "        att = att.masked_fill(self.tril[:time, :time] == 0, float('-inf'))\n",
    "\n",
    "        att = F.softmax(att, dim = -1)\n",
    "        att = self.dropout(att)\n",
    "        out = att @ v\n",
    "\n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for the multi head attention mechanism\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, dimension_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention_heads = nn.ModuleList([AttentionHead(dimension_head) for _ in range (n_heads)])\n",
    "\n",
    "        self.projection_layer = nn.Linear(n_heads * dimension_head, vector_dimension)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = torch.cat([h(x) for h in self.attention_heads], dim =-1)\n",
    "\n",
    "        out = self.dropout(self.projection_layer(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "417f25a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class of a single decoder block\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_heads, vector_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        dimension_head = vector_dimension // n_heads\n",
    "\n",
    "        self.attention = MultiHeadAttention(n_heads, dimension_head)\n",
    "\n",
    "        self.feed_forward = FeedForward(vector_dimension)\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(vector_dimension)\n",
    "\n",
    "        self.layer_norm_2 = nn.LayerNorm(vector_dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        y = self.attention(x)\n",
    "\n",
    "        x = self.layer_norm_1(x + y)\n",
    "\n",
    "        y = self.feed_forward(x)\n",
    "\n",
    "        x = self.layer_norm_2(x + y)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88c2901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model class\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "\n",
    "    # constructor\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, vector_dimension)\n",
    "\n",
    "        self.positional_encodings = nn.Embedding(block_size, vector_dimension)\n",
    "\n",
    "        self.layers = nn.Sequential(*[Block(n_heads, vector_dimension) for _ in range (n_layers)])\n",
    "\n",
    "        self.final_layer_norm = nn.LayerNorm(vector_dimension)\n",
    "\n",
    "        self.linear = nn.Linear(vector_dimension, vocab_size)\n",
    "\n",
    "        self.apply(self.initWeights)\n",
    "\n",
    "    # method to initialize the weights\n",
    "\n",
    "    def initWeights(self, module):\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "\n",
    "            torch.nn.init.normal_(module.weight, std = 0.02)\n",
    "\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "\n",
    "            torch.nn.init.normal_(module.weight, std = 0.02)\n",
    "\n",
    "    # method to forward the next token \n",
    "\n",
    "    def forward(self, index, targets = None):\n",
    "        \n",
    "        batch, time = index.shape\n",
    "\n",
    "        token_embedding = self.token_embeddings(index)\n",
    "\n",
    "        positional_encoding = self.positional_encodings(torch.arange(time, device = device))\n",
    "\n",
    "        x = token_embedding + positional_encoding\n",
    "\n",
    "        x = self.layers(x)\n",
    "\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        if targets == None:\n",
    "\n",
    "            loss = None\n",
    "\n",
    "        else:\n",
    "\n",
    "            batch, time, channels = logits.shape\n",
    "\n",
    "            logits = logits.view(batch * time, channels)\n",
    "\n",
    "            targets = targets.view(batch * time)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    # method to generate the tokens \n",
    "    \n",
    "    def generate(self, index, max_sequence_length):\n",
    "\n",
    "        result = torch.clone(index)\n",
    "\n",
    "        for _ in range(max_sequence_length):\n",
    "\n",
    "            logits, loss = self.forward(index)\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probabilities = F.softmax(logits, dim = -1)\n",
    "\n",
    "            next_index = torch.multinomial(probabilities, num_samples = 1)\n",
    "\n",
    "            index = torch.cat((index, next_index), dim = 1)\n",
    "\n",
    "            result = torch.cat((index, next_index), dim = 1)\n",
    "\n",
    "            a, b = index.shape\n",
    "\n",
    "            if b >= block_size:\n",
    "                index = index[:, 1: ]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b50357fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to optimize and train the model\n",
    "\n",
    "def train(model: GPTModel):\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    avg_training_losses = []\n",
    "\n",
    "    training_losses = []\n",
    "\n",
    "    sum = 0\n",
    "\n",
    "    for i in range (max_iterations):\n",
    "\n",
    "        x, y = getBatch(\"train\")\n",
    "\n",
    "        logits, loss = model.forward(x, y)\n",
    "\n",
    "        sum += loss.item()\n",
    "\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "        avg_training_losses.append(sum/(i+1))\n",
    "\n",
    "        optimizer.zero_grad(set_to_none = True)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % train_step_iteration == 0 :\n",
    "\n",
    "            print(f\"training loss at step {i+1} is: {loss.item(): .5f}\")\n",
    "            print(f\"average training loss at step {i+1} is: {avg_training_losses[-1]: .5f}\")\n",
    "\n",
    "    plt.scatter(np.arange(0, max_iterations), avg_training_losses)\n",
    "    plt.title(\" average training data loss in n loops v/s num loops\")\n",
    "    plt.xticks(np.arange(0, max_iterations+1, 1000))\n",
    "    plt.yticks(np.arange(0, 2.1, 0.1))\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"../graphs/nb/avg_training loss{learning_rate} {model_path[9: -4]}.jpeg\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.scatter(np.arange(0, max_iterations), training_losses)\n",
    "    plt.title(\"training data loss in n loops v/s num loops\")\n",
    "    plt.xticks(np.arange(0, max_iterations+1, 1000))\n",
    "    plt.yticks(np.arange(0, 10.1, 0.5))\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"../graphs/nb/training loss{learning_rate} {model_path[9: -4]}.jpeg\")\n",
    "    plt.show()\n",
    "    \n",
    "    torch.save(model, model_path)\n",
    "    print(\"saved model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "697f432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate the loss\n",
    "\n",
    "@torch.no_grad()\n",
    "\n",
    "def calculateLoss(model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    splits = [\"train\", \"test\"]\n",
    "\n",
    "    for split in splits: \n",
    "\n",
    "        losses = torch.zeros(test_iterations)\n",
    "\n",
    "        for iteration in range (test_iterations):\n",
    "\n",
    "            x, y = getBatch(split)\n",
    "\n",
    "            logits, loss = model.forward(x, y)\n",
    "\n",
    "            losses[iteration] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean()\n",
    "\n",
    "        model.train()\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcc625c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (73728x42 and 504x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m model = torch.load(model_path, weights_only= \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(model_path) \u001b[38;5;28;01melse\u001b[39;00m GPTModel(vocab_size)\n\u001b[32m      3\u001b[39m model = model.to(device)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m train_losses = []\n\u001b[32m      8\u001b[39m test_losses = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m (max_iterations):\n\u001b[32m     15\u001b[39m     x, y = getBatch(\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     logits, loss = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28msum\u001b[39m += loss.item()\n\u001b[32m     21\u001b[39m     training_losses.append(loss.item())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, index, targets)\u001b[39m\n\u001b[32m     44\u001b[39m positional_encoding = \u001b[38;5;28mself\u001b[39m.positional_encodings(torch.arange(time, device = device))\n\u001b[32m     46\u001b[39m x = token_embedding + positional_encoding\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_layer_norm(x)\n\u001b[32m     52\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.linear(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Madhav/Github Repos/gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Madhav/Github Repos/gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Madhav/Github Repos/gpt/lib/python3.11/site-packages/torch/nn/modules/container.py:240\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Madhav/Github Repos/gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Madhav/Github Repos/gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.layer_norm_1(x + y)\n\u001b[32m     24\u001b[39m     y = \u001b[38;5;28mself\u001b[39m.feed_forward(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Madhav/Github Repos/gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Madhav/Github Repos/gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     16\u001b[39m     out = torch.cat([h(x) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.attention_heads])\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprojection_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Madhav/Github Repos/gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Madhav/Github Repos/gpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/Madhav/Github Repos/gpt/lib/python3.11/site-packages/torch/nn/modules/linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (73728x42 and 504x512)"
     ]
    }
   ],
   "source": [
    "model = torch.load(model_path, weights_only= False) if os.path.exists(model_path) else GPTModel(vocab_size)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "train(model)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i in range(max_test_iterations): \n",
    "\n",
    "    loss = calculateLoss(model)\n",
    "    \n",
    "    train_losses.append(loss['train'])\n",
    "    test_losses.append(loss['test'])\n",
    "\n",
    "    if (i+1) % test_step_iterations == 9:\n",
    "\n",
    "        print( f\"at step {i+1} training loss is: {loss['train']: .5f} and testing loss is: {loss['test']: .5f}\")\n",
    "\n",
    "plt.scatter(np.arange(1, max_test_iterations+1), train_losses, color = \"r\", label = \"training set\")\n",
    "plt.scatter(np.arange(1, max_test_iterations+1), test_losses, color = \"g\", label = \"testing set\")\n",
    "    \n",
    "plt.xticks(np.arange(0, max_test_iterations+1, 2 * test_iterations))\n",
    "plt.yticks(np.arange(0, 2, 0.05))\n",
    "\n",
    "plt.xlabel(\"loop num.\")\n",
    "plt.ylabel(\"data loss\")\n",
    "  \n",
    "plt.title(\"loss when testing\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.savefig(f\"../graphs/nb/testing loss{learning_rate} {model_path[9: -4]}.jpeg\")\n",
    "\n",
    "while (True):\n",
    "\n",
    "    prompt = input(\"Enter a prompt within 128 characters\")\n",
    "\n",
    "    context = torch.tensor(encode(prompt), dtype = torch.long, device = device)\n",
    "\n",
    "    context = context.unsqueeze(0)\n",
    "\n",
    "    generated_chars = decode(model.generate(context, max_sequence_length))[0].tolist()\n",
    "\n",
    "    print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
